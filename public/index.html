<!DOCTYPE html>
<html>
   <head>
   <script src="https://code.jquery.com/jquery-3.1.1.min.js"></script>
   <script>
      // global variables
      var leftchannel = [];
      var rightchannel = [];
      var recording = false;
      var recordingLength = 0;
      var sampleRate = null;
      var audioContext = null;


      // Error callback
      function generalError() {
         alert("ERROR!!");
      }

      // success callback when requesting audio input stream
      function gotStream(stream) {
          window.AudioContext = window.AudioContext || window.webkitAudioContext;
          audioContext = new AudioContext();

          // retrieve the current sample rate to be used for WAV packaging
          sampleRate = audioContext.sampleRate;
 
          // creates a gain node
          var volume = audioContext.createGain();

          // Create an AudioNode from the stream.
          var audioIn = audioContext.createMediaStreamSource(stream);

          // connect the stream to the gain node
          audioIn.connect(volume);
  
         // Connect the input stream to the gain node
          audioIn.connect(volume);

          // From the spec: This value controls how frequently the audioprocess event is 
          // dispatched and how many sample-frames need to be processed each call. 
          // Lower values for buffer size will result in a lower(better) latency. 
          // Higher values will be necessary to avoid audio breakup and glitches.
          var bufferSize = 2048;
          var recorder = audioContext.createScriptProcessor(bufferSize, 2, 2);
 
          recorder.onaudioprocess = function(e){
              console.log('recording');
              var left = e.inputBuffer.getChannelData(0);
              var right = e.inputBuffer.getChannelData(1);
              // we clone the samples
              leftchannel.push(new Float32Array(left));
              rightchannel.push(new Float32Array(right));
              recordingLength += bufferSize;
          }
 
          // we connect the recorder
          volume.connect(recorder);
          recorder.connect(audioContext.destination); 

      }

      // Process a wav file

      // 1. Flat down the channels
      function mergeBuffers(channelBuffer, recordingLength){
        var result = new Float32Array(recordingLength);
        var offset = 0;
        var lng = channelBuffer.length;
        for(var i = 0; i < lng; i++){
          var buffer = channelBuffer[i];
          result.set(buffer, offset);
          offset += buffer.length;
        }
        return result;
      }

      // 2. Interleave the audio data
      function interleave(leftChannel, rightChannel){
        var length = leftChannel.length + rightChannel.length;
        var result = new Float32Array(length);
       
        var inputIndex = 0;
 
        for(var index = 0; index < length;){
          result[index++] = leftChannel[inputIndex];
          result[index++] = rightChannel[inputIndex];
          inputIndex++;
        }
        return result;
      }

      // 3. Convert to UTF8
      function writeUTFBytes(view, offset, string){ 
        var lng = string.length;
        for(var i = 0; i < lng; i++){
          view.setUint8(offset + i, string.charCodeAt(i));
        }
      }

      // This is the wav packaging function
      function packageWAVFile() {
         // we flatten the left and right channels down
         var leftBuffer = mergeBuffers(leftchannel, recordingLength);
         var rightBuffer = mergeBuffers(rightchannel, recordingLength);

         // we interleave both channels together
         var interleaved = interleave(leftBuffer, rightBuffer);
          
         // create the buffer and view to create the .WAV file
         var buffer = new ArrayBuffer(44 + interleaved.length * 2);
         var view = new DataView(buffer);
          
         // write the WAV container, check spec at: https://ccrma.stanford.edu/courses/422/projects/WaveFormat/
         // RIFF chunk descriptor
         writeUTFBytes(view, 0, 'RIFF');
         view.setUint32(4, 44 + interleaved.length * 2, true);
         writeUTFBytes(view, 8, 'WAVE');

         // FMT sub-chunk
         writeUTFBytes(view, 12, 'fmt ');
         view.setUint32(16, 16, true);
         view.setUint16(20, 1, true);

         // Is it stereo(2 channels)?
         view.setUint16(22, 2, true);
         view.setUint32(24, sampleRate, true);
         view.setUint32(28, sampleRate * 4, true);
         view.setUint16(32, 4, true);
         view.setUint16(34, 16, true);

         // data sub-chunk
         writeUTFBytes(view, 36, 'data');
         view.setUint32(40, interleaved.length * 2, true);
          
         // write the PCM samples
         var lng = interleaved.length;
         var index = 44;
         var volume = 1;
         for(var i = 0; i < lng; i++){
             view.setInt16(index, interleaved[i] *(0x7FFF * volume), true);
             index += 2;
         }
         console.log("view size = " + index);
         // our final binary blob that we can hand off
         var blob = new Blob([ view ], { type : 'audio/wav' });
	 return blob;
      }

      function sendWav(wavFile){
         var data = new FormData();
         data.append('file', wavFile);
  
         $.ajax({
            url: "https://demoforjohn.mybluemix.net/askwatson",
            type: 'POST',
            data: data,
            contentType: false,
            processData: false,
            success: function(data) {
               console.log("Success!!!");
            }
         });
      }

      function askWatson() {
	 var wavfile = packageWAVFile();
	 sendWav(wavfile);
	 audioContext.close();
	 var elem = document.getElementById("stop");
	 elem.style.display="none";
	 var elem = document.getElementById("start");
	 elem.style.display="block";

      }

      function startRecording() {
	 var elem = document.getElementById("start");
	 elem.style.display="none";
	 var elem = document.getElementById("stop");
	 elem.style.display="block";

	 leftchannel.length = rightchannel.length = 0;
         recordingLength = 0;
         navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mediaDevices.getUserMedia
		                                         || navigator.mozGetUserMedia || navigator.msGetUserMedia;

         navigator.getUserMedia({audio:true}, gotStream, generalError);
      }

   </script>
   </head>
   <body>
      <div id="start" style="display:block">
         <h1>Click button to start ...</h1>
         <input type="button" value="Click to start" onclick="startRecording();">
      </div>
      <div id="stop" style="display:none">
         <h1>Click button to send to Watson ...</h1>
         <input type="button" value="Click to send" onclick="askWatson();">
      </div>
   </body>
</html>
